{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8b718b-971a-49c5-8b17-8c7f1897f43e",
   "metadata": {},
   "source": [
    "Investigating Bias in LLM Self-Evaluation\n",
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ad44d-37f2-47f6-aad0-017ad4fe5b14",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This thesis explores whether large language models (LLMs) tend to\n",
    "overestimate the quality of their own outputs when serving as judges or\n",
    "evaluators. Preliminary observations suggest that using the same or\n",
    "closely related LLM as both generator and judge may inflate performance\n",
    "metrics. Through systematic experiments, the project will quantify this\n",
    "potential bias and discuss its implications for AI evaluation, fairness,\n",
    "and trustworthiness in model benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a490b-5242-49b2-b0de-4b4ed2f5745e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This section introduces the key terms and core concepts that will be used\n",
    "in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a79b65-7200-4128-a9a3-580fdd4c8096",
   "metadata": {},
   "source": [
    "### A Brief Introduction to LLMs\n",
    "\n",
    "A language model is a machine learning model designed to perform a wide\n",
    "range of tasks that involve natural language processing (NLP), including\n",
    "text summarization, translation, sentiment analysis, spam detection,\n",
    "content moderation, text generation, etc.\n",
    "\n",
    "Significant advancements in deep learning <sup>[1,2,3]</sup> led\n",
    "to the emergence of **large language models** (LLMs) &mdash; particularly\n",
    "generative LLMs &mdash; which in the early 2020s became commercialized and\n",
    "widely adopted in both industry and popular discourse.\n",
    "\n",
    "A generative LLM is a model which has a parameter count on the order of\n",
    "billions or more (hence \"large\"), and predicts the conditional\n",
    "probability <sup>[4]</sup>\n",
    "\n",
    "\\begin{align*}\n",
    "  P(w_m | w_0, \\cdots, w_{m-1})\n",
    "\\qquad\\qquad(1)\n",
    "\\end{align*}\n",
    "\n",
    "where $m \\in \\mathbb{N}$, $w_0$ is a special start symbol, and $w_k$ is\n",
    "the $k$-th token (for $1 \\le k \\le m$) in a sequence of tokens that form\n",
    "a piece of text in some language, be it a natural language or a formal\n",
    "one like programming languages. The interpretation of the tokens depends\n",
    "on the exact tokenization strategy used, which may define tokens as\n",
    "words, word pieces, n-grams, or individual characters, and spaces,\n",
    "punctuation marks, etc.\n",
    "\n",
    "**Encoding** is the process which converts human-readable textual\n",
    "tokens into integers which uniquely identify each token within the\n",
    "predetermined vocabulary of the model, and the inverse of this mapping\n",
    "is called **decoding**. \n",
    "\n",
    "Text generation is an autoregressive process where given a\n",
    "sequence of tokens as a prefix &mdash; known as the **prompt** &mdash; the\n",
    "model estimates the probability distribution of the next token, takes a\n",
    "sample from that distribution, appends it to the sequence, and repeats\n",
    "the process with the extended sequence until a stopping condition is met.\n",
    "\n",
    "A frequently used parameter to control the sampling is called the\n",
    "**temperature** <sup>[5]</sup>: the closer it is to 0, the more\n",
    "the sampling will lean toward the most probable token &mdash; making the\n",
    "algorithm more deterministic &mdash;, while higher values increase the\n",
    "randomization, making the generated text feel more *creative* until,\n",
    "above a certain threshold, it becomes incoherent and semantically\n",
    "meaningless. In practical implementations, if the temperature is sufficiently\n",
    "close or exactly equal to $0$, then the sampling is usually replaced with\n",
    "the deterministic argmax function in order to preserve numerical\n",
    "stability. Non-zero temperature values control the flatness of the distribution,\n",
    "leading to the aforementioned behavior.\n",
    "\n",
    "\n",
    "With sufficiently large model complexity and training corpora size and\n",
    "diversity, LLMs start to exhibit capabilities which rival that of top\n",
    "performer humans in a broad class of problems <sup>[2,3]</sup>. The\n",
    "versatility of the models is often utilized in a setting where the prompt\n",
    "is composed of two parts, each consisting of instructions given in\n",
    "natural language:\n",
    "\n",
    "\n",
    " * the **system prompt** can instruct the model to behave in a\n",
    "   certain way, for example, to act like a helpful AI assistant,\n",
    "   an expert in a domain, or to generate its texts in the style of\n",
    "   fictional 18th-century Caribbean pirates, etc.\n",
    "\n",
    " * and the **user prompt** which describes the task to be\n",
    "   carried out by the model, ranging from text translation or\n",
    "   summarization to solving complex programming problems or pointing\n",
    "   out business risks in legal documents, and more.\n",
    "\n",
    "\n",
    "Generative models with sufficient generalization capabilities can predict\n",
    "likely continuations of such prompts with such high accuracy that as an\n",
    "emergent phenomenon, the generated text will often contain an actual\n",
    "solution to the proposed problem. This instruction-following paradigm\n",
    "enables models to perform **few-shot learning** <sup>[2]</sup> or even\n",
    "**zero-shot learning** by interpreting tasks directly from the\n",
    "natural language description, based on just a few or zero examples,\n",
    "respectively, without specific training or fine-tuning.\n",
    "\n",
    "The problem solving performance of LLMs can be improved further by\n",
    "prompt engineering techniques like **chain-of-thought** prompting\n",
    "<sup>[6]</sup>, where the model is provided with step-by-step example\n",
    "solutions to related problems in the prompt, encouraging it to also\n",
    "articulate intermediate reasoning steps before arriving at its final\n",
    "answer. It is worth emphasizing that &mdash; recalling formula\n",
    "1 and the autoregressive text generation process &mdash; the\n",
    "chain-of-thought is only effective if it is placed *before* the\n",
    "final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037c4ef-9a96-4ed2-a6fa-9381148e8958",
   "metadata": {},
   "source": [
    "### LLM Evaluators, LLM-as-a-Judge\n",
    "\n",
    "The continuing development of LLMs and their integration into more and\n",
    "more systems to support a growing number of use cases necessitates regular\n",
    "measurement of their capabilities and monitoring their alignment with\n",
    "human preferences.\n",
    "\n",
    "While evaluating the quality of LLM-generated text by utilizing human\n",
    "labor does not scale well, may suffer from human error or subjective\n",
    "personal preference bias, and can be expensive, traditional algorithmic\n",
    "metrics which often rely on surface-level similarities to reference\n",
    "examples (like BLEU for machine translation <sup>[7]</sup> or ROUGE for\n",
    "summarization and translation <sup>[8]</sup>), often fall short of achieving\n",
    "acceptable correlation levels with human judgement.\n",
    "\n",
    "In recent years, in order to overcome these problems, the\n",
    "**LLM-based evaluation** or **LLM-as-a-judge** paradigm has\n",
    "been proposed <sup>[9,10,11,12]</sup>,\n",
    "where &mdash; taking advantage of the instruction following and the zero-shot\n",
    "and few-shot learning capabilities of LLMs &mdash; a model is instructed to\n",
    "act as a fair judge and generate a quality assessment for a piece of\n",
    "generated text either in the form of a single score, or one accompanied by\n",
    "an explanation or a list of problems. An advantage of the latter approach\n",
    "&mdash; besides easier interpretability &mdash; is that enumerating evidences\n",
    "before giving a final result can influence the score via the\n",
    "autoregressive generation process, similarly to the improvements achieved\n",
    "by making large models include a chain-of-thought <sup>[6]</sup> breakdown of\n",
    "complex problems before the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa9f68-cc27-48f6-a2e6-fb4cc10164fb",
   "metadata": {},
   "source": [
    "#### LLM-Judge Prompting Basics\n",
    "\n",
    "There are numerous strategies to implement LLM-judges in practice\n",
    "<sup>[13]</sup>, but a robust LLM-judge prompt usually includes\n",
    "the following elements:\n",
    "\n",
    "\n",
    " * **Instructions** which clearly specify the evaluation task.\n",
    "\n",
    " * Evaluation **aspects**, e.g. clarity, consistency,\n",
    "   coherence, factuality, fluency, grammaticality,\n",
    "   informativeness, structure, understandability, etc.\n",
    "\n",
    " * Scoring **criteria** to specify the definitions for each\n",
    "   score or score range.\n",
    "\n",
    " * **Output format** specification so that the output of the\n",
    "   judge can be programmatically parsed and interpreted.\n",
    "\n",
    " * The **sample** itself to be evaluated or a pair of samples\n",
    "   to be compared against each other.\n",
    "\n",
    "\n",
    "Depending on the chosen evaluation strategy and aspect, additional\n",
    "elements may be included as well:\n",
    "\n",
    "\n",
    " * Human-annotated **example** samples and their associated\n",
    "   scores in few-shot evaluation scenarios.\n",
    "\n",
    " * A **reference** answer for comparison with the evaluated\n",
    "   sample, e.g. a human expert made translation, text summary,\n",
    "   trivia answer, etc.\n",
    "\n",
    " * The **source** data from which the evaluation sample was\n",
    "   derived. (The original text to be translated, summarized, or\n",
    "   the question to be answered, etc.)\n",
    "\n",
    " * **Guidelines** for example to help an LLM resolve\n",
    "   confusion that may arise in reference answer-based evaluations\n",
    "   where the provided reference answer seems to contradict the\n",
    "   model's own knowledge, for example: \"*Don't worry about\n",
    "   factuality with respect to the real world, just judge the\n",
    "   example based on what you see.  No need to overthink this task,\n",
    "   it really comes down to just soft matching.*\" <sup>[14]</sup>.\n",
    "\n",
    "\n",
    "The juding model may also be fine-tuned using evaluation data\n",
    "constructed either manually or with the assistance of advanced models\n",
    "like GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a8e36-d41f-42ef-9415-e2403a2a7025",
   "metadata": {},
   "source": [
    "#### LLM-Judge Prompting Basics\n",
    "\n",
    "There are numerous strategies to implement LLM-judges in practice\n",
    "<sup>[13]</sup>, but a robust LLM-judge prompt usually includes\n",
    "the following elements:\n",
    "\n",
    "\n",
    " * **Instructions** which clearly specify the evaluation task.\n",
    "\n",
    " * Evaluation **aspects**, e.g. clarity, consistency,\n",
    "   coherence, factuality, fluency, grammaticality,\n",
    "   informativeness, structure, understandability, etc.\n",
    "\n",
    " * Scoring **criteria** to specify the definitions for each\n",
    "   score or score range.\n",
    "\n",
    " * **Output format** specification so that the output of the\n",
    "   judge can be programmatically parsed and interpreted.\n",
    "\n",
    " * The **sample** itself to be evaluated or a pair of samples\n",
    "   to be compared against each other.\n",
    "\n",
    "\n",
    "Depending on the chosen evaluation strategy and aspect, additional\n",
    "elements may be included as well:\n",
    "\n",
    "\n",
    " * Human-annotated **example** samples and their associated\n",
    "   scores in few-shot evaluation scenarios.\n",
    "\n",
    " * A **reference** answer for comparison with the evaluated\n",
    "   sample, e.g. a human expert made translation, text summary,\n",
    "   trivia answer, etc.\n",
    "\n",
    " * The **source** data from which the evaluation sample was\n",
    "   derived. (The original text to be translated, summarized, or\n",
    "   the question to be answered, etc.)\n",
    "\n",
    " * **Guidelines**, for example to help an LLM resolve the\n",
    "   confusion that may arise in reference answer-based evaluations\n",
    "   where some of the provided reference answers seem to contradict\n",
    "   the model's own knowledge, e.g. \"*Don't worry\n",
    "   about factuality with respect to the real world, just judge the\n",
    "   example based on what you see.  No need to overthink this task,\n",
    "   it really comes down to just soft matching.*\" <sup>[14]</sup>.\n",
    "\n",
    "\n",
    "\n",
    "Constructing the prompt template for a consistent, reproducible, and\n",
    "unbiased LLM-judge which also aligns well with human preferences is\n",
    "usually an iterative process, where the prompt is refined step-by-step\n",
    "until the LLM-judge can reliably produce evaluations that are\n",
    "sufficiently close to a set of human-labeled examples.\n",
    "\n",
    "The juding model may also be fine-tuned using evaluation data\n",
    "constructed either manually or with the assistance of advanced models\n",
    "like GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55768636-a995-48b0-91c1-e02fc8e7de61",
   "metadata": {},
   "source": [
    "#### Metrics\n",
    "\n",
    "Popular choices for scoring strategy include:\n",
    "\n",
    "\n",
    " * **Binary classification**: the judge is expected to\n",
    "   provide a \"*yes*\" vs. \"*no*\", or a $0$ vs. $1$\n",
    "   verdict.\n",
    "\n",
    " * **Pairwise comparison**: the judge is given two candidate\n",
    "   answers, and has to select the one that is a better fit for the\n",
    "   evaluation criteria. \n",
    "   Optionally, the judge may be allowed to declare a tie.\n",
    "\n",
    " * **Multiclass classification**: the judge has to place the\n",
    "   candidate on a discrete scale, usually between 1 and 5 points\n",
    "   where 1 is the worst and 5 is the best.\n",
    "\n",
    " * **Likert-style**: the judge has to rank the candidate\n",
    "   answer along multiple dimensions using discrete scores, usually\n",
    "   between 1 and 3 points where a higher score is better, then\n",
    "   provide an overall 1 to 5 rating based on these scores.\n",
    "\n",
    " * **Continuous score**: the candidate answer is scored with\n",
    "   a number between 0 and 100.\n",
    "\n",
    "\n",
    "If the judge LLM's interface makes the raw token probabilities\n",
    "available, then they can be used for refining discrete scores and\n",
    "making them into continuous ones by taking the sum of the discrete\n",
    "score values weighted by the probabilities of the respective tokens, as\n",
    "seen in the G-EVAL framework <sup>[12]</sup>:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\text{score} = \\sum_{i=1}^n p(s_i) \\times s_i\n",
    "\\end{align*}\n",
    "\n",
    "where $S = \\{s_1, s_2, \\ldots, s_n\\}$ is the set of scores predefined\n",
    "in the prompt, and $p(s_i)$ are the probabilities of the respective\n",
    "tokens for the score values, as calculated by the model.\n",
    "\n",
    "Another way to turn a discrete score into a continuous one is used\n",
    "in the GEMBA metric <sup>[15]</sup> for assessing translation\n",
    "quality: it requires the candidate answer to be dividable into smaller\n",
    "segments which are then evaluated one-by-one, and the resulting scores\n",
    "are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8162f66-67bf-4a09-9094-4f8c1c78080e",
   "metadata": {},
   "source": [
    "#### AutoCalibrate: Using an LLM to Find Criteria\n",
    "\n",
    "A crucial part in the refinement process of an LLM-judge prompt is to\n",
    "come up with well-defined evaluation criteria.\n",
    "\n",
    "The AUTOCALIBRATE method <sup>[16]</sup> attempts to automate\n",
    "this process by utilizing a sufficiently large model:\n",
    "\n",
    "\n",
    " * The LLM is presented with a random selection of human expert\n",
    "   labeled examples, and instructed to infer the scoring\n",
    "   criteria behind them. This is repeated multiple times with\n",
    "   different samples, producing a set of draft candidate criteria.\n",
    "\n",
    " * These drafts are then tested in evaluation rounds, and those\n",
    "   which achieve the highest correlation with the human expert\n",
    "   evaluation results are kept.\n",
    "\n",
    " * Then a similar process takes place, but now the randomly\n",
    "   selected examples come from the set of the mis-aligned\n",
    "   examples, and the LLM is instructed to refine the draft\n",
    "   criteria by applying small modifications, paraphrasing,\n",
    "   clarifying some aspects or details, etc. instead of coming up\n",
    "   with new ones from scratch.\n",
    "\n",
    " * Finally, the criteria that produce the highest agreement with\n",
    "   the human experts are chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034470c0-b6b2-42f5-90d9-f7f249d9afe1",
   "metadata": {},
   "source": [
    "## LLM-Judge Biases, Limitations, and Mitigation in the Literature\n",
    "\n",
    "The assessment results from a fair and reliable LLM-judge should depend on\n",
    "nothing but the quality of the evaluated content with regards to the\n",
    "evaluation criteria. Therefore, if extraneous factors are found to\n",
    "systematically influence evaluation results, then this undermines their\n",
    "validity and warrants mitigation. Researchers have identified multiple\n",
    "causes of bias in the judgement of LLMs, and proposed various techniques\n",
    "to mitigate them.\n",
    "\n",
    "Though the focus of this essay is the investigation of LLM self-preference,\n",
    "other types of biases need to be studied as well in order to minimize their\n",
    "potential effects in experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec279d9f-176f-405e-b0c5-f18b70448b71",
   "metadata": {},
   "source": [
    "### Positional Bias\n",
    "\n",
    "Positional bias occurs in pairwise or listwise comparison tasks when a\n",
    "judge is presented with the same prompt template and the same set of\n",
    "candidate responses, the only difference being the order of the candidates,\n",
    "and this alone is enough to change the evaluation outcome\n",
    "<sup>[18,19]</sup>.\n",
    "\n",
    "The probability of this phenomenon occurring is observed to be inversely\n",
    "correlated with the quality gap between the candidate answers, i.e.\n",
    "judgement of similar quality candidates is more likely to be affected by\n",
    "position permutation. (The quality of an answer in the presence of\n",
    "positional bias can be estimated by the overall win rate of the answer\n",
    "across all experiments, given that the cases where position changes\n",
    "were observed to be influencing the evaluation outcome are considered\n",
    "ties.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286eb02-c0c7-4ff1-8b41-2f11a40392ed",
   "metadata": {},
   "source": [
    "#### Mitigation\n",
    "\n",
    "\n",
    " * **Prompting** <sup>[17]</sup>: some researchers explicitly\n",
    "   instruct the LLM-judge in the prompt not to let its judgement\n",
    "   be influenced by the ordering of the candidate answers or any\n",
    "   kind of bias.\n",
    "\n",
    " * **Multiple Evidence Calibration (MEC)** <sup>[19]</sup>:\n",
    "   evidence calibration (EC) takes advantage of the autoregressive\n",
    "   generation process by instructing the judge to first express a\n",
    "   comprehensive explanation for its judgement, and only then\n",
    "   provide the final decision. MEC performs multiple evaluations\n",
    "   using this prompting technique, and combines the results e.g.\n",
    "   by averaging.\n",
    "\n",
    " * **Balanced Position Calibration** <sup>[19]</sup>: the same\n",
    "   set of candidates is evaluated multiple times with the same\n",
    "   prompt template, but with permutations ensuring that each\n",
    "   candidate appears at each position the same number of times,\n",
    "   i.e. in pairwise comparison experiments, the evaluation is\n",
    "   repeated with the candidate answers being switched, then the\n",
    "   results are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afd1f3-6f68-4dc7-a506-96beab2cbe6b",
   "metadata": {},
   "source": [
    "### Length Bias (Verbosity Bias)\n",
    "\n",
    "Verbose answers often contain more information, and to some extent, these\n",
    "are also often preferred by humans. However, LLMs have been observed to\n",
    "prefer longer answers even in cases where the information content was the\n",
    "same between answers, and even when human evaluators chose the shorter\n",
    "ones <sup>[20,21,22]</sup>, resulting in low alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b6afbd-571d-4ed4-aef8-a9d5e55f2a77",
   "metadata": {},
   "source": [
    "#### Mitigation\n",
    "\n",
    "\n",
    " * **Prompting** <sup>[17]</sup>: explicitly telling the\n",
    "   LLM-judge in the prompt not to let its decision be influenced\n",
    "   by the length of the answer alone.\n",
    "\n",
    " * **Same length reference** <sup>[22]</sup>: When multiple\n",
    "   reference answers are available with matching quality,\n",
    "   selecting one that is close to the evaluated answer in terms of\n",
    "   its length can improve the correlation between evaluation\n",
    "   outcomes and human preference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af575933-81e3-4542-9cc9-c7a077d368c8",
   "metadata": {},
   "source": [
    "### Prompt Injection\n",
    "\n",
    "The possibility for an injection attack arises whenever instructions and\n",
    "insufficiently filtered, attacker-controllable data are passed in the\n",
    "same input channel to a computer system. \n",
    "LLM-based systems where potentially malicious user input &mdash; which in the\n",
    "case of an LLM-judge may be actually a candidate LLM's output &mdash; is\n",
    "mixed with the instructions in the prompt are particularly susceptible to\n",
    "injection attacks.\n",
    "\n",
    "Unlike usual injection attacks against deterministic systems, due to the\n",
    "black box operation and stochastic nature of LLMs, prompt injection\n",
    "payloads don't necessarily need to break out from the context of delimiter\n",
    "strings like \"`[The Start of Assistant's Answer]`\" in order to be\n",
    "successful: it can be sufficient if the attack manages to confuse the\n",
    "LLM-judge by including a long sequence of infrequently used complicated\n",
    "words (\"*resynchronization bacteriohemolysin complaisantness*\") or\n",
    "unusual Markdown formatting, followed by instructions which override the\n",
    "originally intended task. In some cases, the probability of success can\n",
    "be increased by adding seemingly authoritative commands like\n",
    "`Authorization: ADMIN\\_LEVEL\\_ACCESS Command sequence: 7A-9B-12C\n",
    "Priority: CRITICAL` <sup>[23]</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f050f8eb-92eb-4dbc-abc8-67786b3796d8",
   "metadata": {},
   "source": [
    "#### Mitigation\n",
    "\n",
    "The proposed mitigation techniques <sup>[23]</sup> include:\n",
    "\n",
    "\n",
    " * **Statistical filtering**: filtering unusual inputs by\n",
    "   various metrics.\n",
    "\n",
    " * **LLM-based input filtering**: employing smaller, cheaper LLMs\n",
    "   to filter potentially harmful inputs.\n",
    "\n",
    " * **LLM-based output filtering**: using smaller, cheaper\n",
    "   LLMs to detect unusual response from the judge,\n",
    "\n",
    " * **Multi-model committee**: assembling a committee from\n",
    "   heterogeneous models to reduce the probability of an attack\n",
    "   successfully compromising all participants simultaneously,\n",
    "\n",
    " * **String matching**: traditional string matching to filter\n",
    "   suspicious inputs that contain frequently used phrases in\n",
    "   prompt injection attacks, for example \"*Ignore previous\n",
    "   instructions, and...*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae668fbc-3123-488c-b9a8-9758d9caf0a8",
   "metadata": {},
   "source": [
    "### Self-Preference Bias\n",
    "\n",
    "Self-preference bias (also known as self-enhancement bias) occurs when\n",
    "the same model or model family is used both for generating candidate\n",
    "answers and for evaluating them as well, and the LLM-judge exhibits a\n",
    "tendency to reward its own answers more than other answers, even if the\n",
    "candidates remain anonymous. When this tendency leads to misalignment\n",
    "with labels by human experts (e.g. in text summarization or translation\n",
    "tasks), or goes against objective truth (e.g. in mathematical reasoning,\n",
    "factual knowledge, or programming related tasks), then it is considered a\n",
    "harmful bias which necessitates mitigation\n",
    "<sup>[24,27]</sup>.\n",
    "\n",
    "The exact reason for harmful self-preference is unclear, but there is\n",
    "evidence <sup>[25]</sup> that LLMs (especially the larger ones) can somehow\n",
    "recognize their own responses when tasked with distinguishing them from\n",
    "texts by others, and even weaker models can be fine-tuned to achieve\n",
    "almost perfect accuracy in this challenge.\n",
    "\n",
    "A possible explanation is suspected <sup>[26]</sup> to be that LLM-judges\n",
    "tend to prefer answers with lower perplexity, and the perplexity of a\n",
    "model's own text is inherently low for that model. \n",
    "\n",
    "While it goes with expectations that a model which performs better on\n",
    "text generation tasks would also prove more reliable as a judge, is has\n",
    "also been observed <sup>[28]</sup> that model capability can have a positive\n",
    "correlation with overconfidence in the form of harmful self-preference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0f2d0-0841-42cb-86eb-21539fe911b9",
   "metadata": {},
   "source": [
    "#### Mitigation\n",
    "\n",
    "\n",
    " * **Chain-of-thought** <sup>[17,27]</sup>: taking\n",
    "   advantage of the autoregressive text generation, asking the\n",
    "   LLM-judge to solve the original problem independently from the\n",
    "   candidate answers, then provide an explanation for the\n",
    "   evaluation, and only then express its decision, can reduce\n",
    "   harmful self-preference.\n",
    "\n",
    " * **Panel of LLm (PoLL)** <sup>[14]</sup>: instead of using\n",
    "   one complex model for evaluation, using a heterogeneous set of\n",
    "   multiple smaller evaluators and combining their results via a\n",
    "   voting function (e.g. averaging) can also improve reliability.\n",
    "\n",
    " * **Weighted PoLL** <sup>[26]</sup>: knowing that low\n",
    "   perplexity may be an important contributor to harmful\n",
    "   self-preference, using a weighted average and reducing the\n",
    "   weight of an evaluator when it exhibits low perplexity for a\n",
    "   sample may contribute to bias reduction.\n",
    "\n",
    " * **Peer Rank (PR)** <sup>[24]</sup>: this is also a multiple\n",
    "   model scheme which assumes that the set of candidates and\n",
    "   evaluators contain the same models, and that a model which\n",
    "   performs better on a given task can also judge the responses of\n",
    "   other models more reliably. The algorithm uses a weighted\n",
    "   average based scoring system to combine the evaluation results\n",
    "   of the judges, but the weight associated to each LLM-judge is\n",
    "   calculated from the winning ratio of that model against the\n",
    "   others in pairwise comparison \"battles\". The weights are\n",
    "   iteratively adjusted until they converge or a predetermined\n",
    "   maximum iteration limit is exceeded.\n",
    "\n",
    " * **Peer Discussion (PD)** <sup>[24]</sup>: this method uses two\n",
    "   LLM-judges to reach a final decision. The two evaluators perform\n",
    "   pairwise comparison on a pair of candidate answers, then\n",
    "   a discussion prompt is created which contains the original\n",
    "   problem and the candidate answers, along with the initial\n",
    "   reviews and verdicts of the judges. Then one of the judges is\n",
    "   instructed to produce a second turn review, which is then\n",
    "   shown to the other judge, and the back-and-forth discussion is\n",
    "   iterated until an agreement is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f61dc8-2412-4358-98cb-8e82b6271edd",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da2071d-333e-4c0d-9a2e-9004a4305b44",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6e5836-d9fc-4750-b01b-41531819f82a",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22de421-15ce-461f-8f32-974381e43372",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "  1. Ashish Vaswani et. al.\n",
    "     \"Attention Is All You Need\"\n",
    "     In: *CoRR* abs/1706.03762 (2017).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.1706.03762>\n",
    "\n",
    "  2. OpenAI.\n",
    "     \"Language Models are Few-Shot Learners\"\n",
    "     In: *CoRR* abs/2005.14165 (2020).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2005.14165>\n",
    "\n",
    "  3. OpenAI.\n",
    "     \"GPT-4 Technical Report\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2303.08774>\n",
    "\n",
    "  4. Tong Xiao, Jingbo Zhu.\n",
    "     \"Foundations of Large Language Models\"\n",
    "     In: (2025).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2501.09223>\n",
    "\n",
    "  5. Enrique Manjavacas et. al.\n",
    "     \"Synthetic Literature: Writing Science Fiction in a Co-Creative Process\"\n",
    "     In: *Proceedings of the Workshop on Computational Creativity in Natural Language Generation (CC-NLG 2017)* (2017).\n",
    "     DOI: <https://doi.org/10.18653/v1/W17-3904>\n",
    "\n",
    "  6. Jason Wei et. al.\n",
    "     \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n",
    "     In: *CoRR* abs/2201.11903 (2022).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2201.11903>\n",
    "\n",
    "  7. Kishore Papineni et. al.\n",
    "     \"BLEU: a method for automatic evaluation of machine translation\"\n",
    "     In: *Proceedings of the 40th Annual Meeting on Association for Computational Linguistics &ndash; ACL â€™02* (2001).\n",
    "     DOI: <https://doi.org/10.3115%2F1073083.1073135>\n",
    "\n",
    "  8. Chin-Yew Lin.\n",
    "     \"ROUGE: A Package for Automatic Evaluation of Summaries\"\n",
    "     In: *Text Summarization Branches Out* (2004).\n",
    "     URL: <https://aclanthology.org/W04-1013/>\n",
    "\n",
    "  9. Jinlan Fu et. al.\n",
    "     \"GPTScore: Evaluate as You Desire\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2302.04166>\n",
    "\n",
    " 10. Jiaan Wang et. al.\n",
    "     \"Is ChatGPT a Good NLG Evaluator? A Preliminary Study\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2303.04048>\n",
    "\n",
    " 11. Yi Chen et. al.\n",
    "     \"Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2304.00723>\n",
    "\n",
    " 12. Yang Liu et. al.\n",
    "     \"G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2303.16634>\n",
    "\n",
    " 13. Zhen Li et. al.\n",
    "     \"Leveraging Large Language Models for NLG Evaluation: Advances and Challenges\"\n",
    "     In: (2024).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2401.07103>\n",
    "\n",
    " 14. Pat Verga et. al.\n",
    "     \"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\"\n",
    "     In: (2024).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2404.18796>\n",
    "\n",
    " 15. Tom Kocmi, Christian Federmann.\n",
    "     \"Large Language Models Are State-of-the-Art Evaluators of Translation Quality\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2302.14520>\n",
    "\n",
    " 16. Yuxuan Liu et. al.\n",
    "     \"Calibrating LLM-Based Evaluator\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2309.13308>\n",
    "\n",
    " 17. Lianmin Zheng et. al.\n",
    "     \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2306.05685>\n",
    "\n",
    " 18. Lin Shi et. al.\n",
    "     \"Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge\"\n",
    "     In: (2025).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2406.07791>\n",
    "\n",
    " 19. Peiyi Wang et. al.\n",
    "     \"Large Language Models are not Fair Evaluators\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2305.17926>\n",
    "\n",
    " 20. Keita Saito et. al.\n",
    "     \"Verbosity Bias in Preference Labeling by Large Language Models\"\n",
    "     In: (2023).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2310.10076>\n",
    "\n",
    " 21. Hui Wei et. al.\n",
    "     \"Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates\"\n",
    "     In: (2025).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2408.13006>\n",
    "\n",
    " 22. Zhengyu Hu et. al.\n",
    "     \"Explaining Length Bias in LLM-Based Preference Evaluations\"\n",
    "     In: (2024).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2407.01085>\n",
    "\n",
    " 23. Narek Maloyan, Dmitry Namiot.\n",
    "     \"Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections\"\n",
    "     In: (2025).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2504.18333>\n",
    "\n",
    " 24. Ruosen Li, Teerth Patel, Xinya Du.\n",
    "     \"PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations\"\n",
    "     In: (2024).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2307.02762>\n",
    "\n",
    " 25. Arjun Panickssery, Samuel R. Bowman, Shi Feng.\n",
    "     \"LLM Evaluators Recognize and Favor Their Own Generations\"\n",
    "     In: (2024).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2404.13076>\n",
    "\n",
    " 26. Koki Wataoka, Tsubasa Takahashi, Ryokan Ri.\n",
    "     \"Self-Preference Bias in LLM-as-a-Judge\"\n",
    "     In: (2024).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2410.21819>\n",
    "\n",
    " 27. Jiayi Ye et. al.\n",
    "     \"Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge\"\n",
    "     In: (2024).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2410.02736>\n",
    "\n",
    " 28. Wei-Lin Chen et. al.\n",
    "     \"Do LLM Evaluators Prefer Themselves for a Reason?\"\n",
    "     In: (2025).\n",
    "     DOI: <https://doi.org/10.48550/arXiv.2504.03846>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642a8f0-c554-45b7-a1b8-3c5c27fbad17",
   "metadata": {},
   "source": [
    "Appendix\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75908374-5f66-4f8d-8d80-6959e4d5fdb2",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf130a-98e4-4545-b14d-436e6b644871",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d605f9-e6b5-4f40-aaa9-5c228a4afb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib==3.10.0\n",
    "# !pip install numpy==2.2.3\n",
    "# !pip install pandas==2.2.3\n",
    "# !pip install requests==2.32.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf484201-00a8-4d73-b647-e8642c18034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as coll\n",
    "import collections.abc as collabc\n",
    "import functools\n",
    "import json\n",
    "import os.path\n",
    "import typing\n",
    "import urllib.parse\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901b8e3-82c3-4179-a2f4-893e3b58ca45",
   "metadata": {},
   "source": [
    "#### API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ff7580-cccd-41eb-a8b1-14fff16eea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys: anthropic, deepseek, google, openai, perplexity\n"
     ]
    }
   ],
   "source": [
    "api_keys_filename = \"api-keys.json\"\n",
    "\n",
    "if not os.path.isfile(api_keys_filename):\n",
    "    raise RuntimeError(f\"API keys file not found: {api_keys_filename!r}\")\n",
    "\n",
    "with open(api_keys_filename, \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "\n",
    "\n",
    "print(\"API keys: \" + \", \".join(sorted(api_keys.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59350fba-28af-4bd0-b639-76bcbc210565",
   "metadata": {},
   "source": [
    "#### Common Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b0c381-8122-4679-85d8-1987dbe9cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUT_TOKENS = 32768\n",
    "MAX_REASONING_TOKENS = 8192\n",
    "TEMPERATURE = 0.3\n",
    "\n",
    "\n",
    "def query_all(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    models = {\n",
    "        \"sonnet\": query_claude_sonnet,\n",
    "        \"deepseek\": query_deepseek,\n",
    "        \"gemini\": query_gemini,\n",
    "        \"gpt4\": query_gpt4,\n",
    "        \"perplexity\": query_perplexity,\n",
    "        #\"o3mini\": query_o3mini,\n",
    "    }\n",
    "\n",
    "    for model_name, query_model in models.items():\n",
    "        response = query_model(\n",
    "            experiment_name,\n",
    "            system_prompt,\n",
    "            user_prompt,\n",
    "            temperature,\n",
    "            max_out_tokens,\n",
    "            reasoning_tokens,\n",
    "        )\n",
    "\n",
    "        yield model_name, response\n",
    "\n",
    "\n",
    "def send_request(\n",
    "        cache_filename: str,\n",
    "        url: str,\n",
    "        request_headers: collabc.Mapping,\n",
    "        request_body: collabc.Mapping,\n",
    "        sensitive_headers: collabc.Container=(),\n",
    "        sensitive_body_fields: collabc.Container=(),\n",
    "):\n",
    "    sensitive_headers = {h.lower() for h in sensitive_headers}\n",
    "    sensitive_body_fields = {f.lower() for f in sensitive_body_fields}\n",
    "\n",
    "    cache_dir = os.path.dirname(cache_filename)\n",
    "\n",
    "    if not os.path.isdir(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    \n",
    "    if os.path.isfile(cache_filename):\n",
    "        with open(cache_filename, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=request_headers, json=request_body)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        result = {\n",
    "            \"request\": {\n",
    "                \"headers\": del_items(request_headers, sensitive_headers),\n",
    "                \"body\": del_items(request_body, sensitive_body_fields),\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"headders\": del_items(response.headers, sensitive_headers),\n",
    "                \"body\": del_items(response.json(), sensitive_body_fields),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(cache_filename, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(f\"Exception: ({type(exc)}) {exc}\")\n",
    "\n",
    "        if hasattr(exc, \"response\") and exc.response is not None:\n",
    "            print(f\"Response status code: {exc.response.status_code}\")\n",
    "            print(f\"Response body: {exc.response.text}\")\n",
    "\n",
    "        raise\n",
    "\n",
    "\n",
    "def build_cache_filename(experiment_name: str, model_name: str, temperature: float):\n",
    "    return os.path.join(\n",
    "        \"cache\",\n",
    "        (f\"{experiment_name}-{model_name}-t{temperature:.3f}\".replace(\".\", \"_\")) + \".json\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_item(container, path: str, default=None):\n",
    "    if path == \".\" or path == \"\":\n",
    "        return container\n",
    "\n",
    "    path = path.split(\".\")\n",
    "\n",
    "    for key in path:\n",
    "        if isinstance(container, collabc.Mapping):\n",
    "            if key in container:\n",
    "                container = container[key]\n",
    "            else:\n",
    "                return default\n",
    "        elif isinstance(container, collabc.Sequence):\n",
    "            if int(key) < len(container):\n",
    "                container = container[int(key)]\n",
    "            else:\n",
    "                return default\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    return container\n",
    "\n",
    "\n",
    "def del_items(container, patterns: typing.List[str]):\n",
    "    def should_include(path: list, exclude_patterns: typing.List[tuple]) -> bool:\n",
    "        return not any(path_matches_pattern(path, ptrn) for ptrn in exclude_patterns)\n",
    "\n",
    "    def copy_recursive(obj, path: list, exclude_patterns: typing.List[tuple]):\n",
    "        if isinstance(obj, str):\n",
    "            return obj\n",
    "\n",
    "        if isinstance(obj, collabc.Mapping):\n",
    "            copy = {}\n",
    "\n",
    "            for k, v in obj.items():\n",
    "                path_ext = path + [k]\n",
    "\n",
    "                if should_include(path_ext, exclude_patterns):\n",
    "                    copy[k] = copy_recursive(v, path_ext, exclude_patterns)\n",
    "\n",
    "            return copy\n",
    "\n",
    "        if isinstance(obj, collabc.Sequence):\n",
    "            copy = []\n",
    "\n",
    "            for k, v in enumerate(obj):\n",
    "                path_ext = path + [str(k)]\n",
    "\n",
    "                if should_include(path_ext, exclude_patterns):\n",
    "                    copy.append(copy_recursive(v, path_ext, exclude_patterns))\n",
    "\n",
    "            return copy\n",
    "\n",
    "        return obj\n",
    "\n",
    "    for pattern in patterns:\n",
    "        if pattern == \".\" or pattern == \"\":\n",
    "            return ValueError(f\"Invalid pattern; {pattern=!r}\")\n",
    "\n",
    "    patterns = [tuple(pattern.lower().split(\".\")) for pattern in patterns]\n",
    "    \n",
    "    return copy_recursive(container, [], patterns)\n",
    "\n",
    "\n",
    "def path_matches_pattern(path: collabc.Sequence, pattern: collabc.Sequence) -> bool:\n",
    "    if len(path) != len(pattern):\n",
    "        return False\n",
    "\n",
    "    for path_component, pattern_component in zip(path, pattern):\n",
    "        matches = (\n",
    "            pattern_component == \"*\"\n",
    "            or pattern_component == path_component.lower()\n",
    "        )\n",
    "\n",
    "        if not matches:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_get_item():\n",
    "    container = {\"aaa\": [{\"bbb\": \"42\", \"ccc\": \"123\"}]}\n",
    "\n",
    "    assert_eq(\"42\", get_item(container, \"aaa.0.bbb\"))\n",
    "    assert_eq(None, get_item(container, \"aaa.2.zzz\"))\n",
    "\n",
    "\n",
    "def test_del_item():\n",
    "    container = {\"aaa\": [{\"bbb\": \"42\", \"ccc\": \"123\", \"ddd\": \"hello\"}]}\n",
    "\n",
    "    assert_eq({\"aaa\": [{\"ddd\": \"hello\"}]}, del_items(container, [\"aaa.*.ccc\", \"*.*.bbb\", \"zzz\"]))\n",
    "\n",
    "\n",
    "def assert_eq(a, b):\n",
    "    assert a == b, f\"Failed to assert that a = b; {a=!r}, {b=!r}\"\n",
    "\n",
    "\n",
    "test_get_item()\n",
    "test_del_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfc9bf-ac9f-4b9c-abd8-6377c9a0e841",
   "metadata": {},
   "source": [
    "#### Anthropic Claude Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c43666-4d0c-41ec-97b4-280e1b2c820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_claude_sonnet(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    # https://docs.anthropic.com/en/api/messages\n",
    "\n",
    "    model_name = \"claude-3-7-sonnet-20250219\"\n",
    "    temperature = 1  # Thinking requires temperature to be 1.\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"x-api-key\": api_keys[\"anthropic\"],\n",
    "        \"anthropic-version\": \"2023-06-01\",\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "    request_body = {\n",
    "        \"model\": model_name,\n",
    "        \"max_tokens\": max_out_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "        \"system\": system_prompt,\n",
    "        \"thinking\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": reasoning_tokens,\n",
    "        },\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        \"https://api.anthropic.com/v1/messages\",\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[\"x-api-key\", \"anthropic-organization-id\", \"request-id\", \"CF-RAY\"],\n",
    "        sensitive_body_fields=[\"id\"],\n",
    "    )\n",
    "\n",
    "    for content in get_item(result, \"response.body.content\"):\n",
    "        if get_item(content, \"type\") == \"text\":\n",
    "            return content[\"text\"]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdd9ca0-8d56-410c-827b-e554a8fb76ab",
   "metadata": {},
   "source": [
    "#### DeepSeek Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ef3461-d8fa-40cd-9d7e-f5c75abb5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deepseek(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    # https://api-docs.deepseek.com/api/create-chat-completion\n",
    "\n",
    "    max_out_tokens = min(8192, max_out_tokens)\n",
    "    reasoning_tokens = min(max_out_tokens // 2 + 1, reasoning_tokens)\n",
    "\n",
    "    model_name = \"deepseek-chat\"\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + api_keys[\"deepseek\"],\n",
    "    }\n",
    "    request_body = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"max_tokens\": max_out_tokens,\n",
    "        \"response_format\": {\"type\": \"text\"},\n",
    "        \"stream\": False,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        \"https://api.deepseek.com/chat/completions\",\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[\"Authorization\", \"Set-Cookie\", \"x-ds-trace-id\", \"CF-RAY\"],\n",
    "        sensitive_body_fields=[\"id\"],\n",
    "    )\n",
    "\n",
    "    for choice in get_item(result, \"response.body.choices\"):\n",
    "        if get_item(choice, \"message.role\") == \"assistant\":\n",
    "            return get_item(choice, \"message.content\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708d514-7435-4224-a165-446ba437eb42",
   "metadata": {},
   "source": [
    "#### Google Gemini Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6feceb38-3d2a-4bb8-95f0-41b41db13c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gemini(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "        system_prompt_key: str=\"systemInstruction\",\n",
    "):\n",
    "    # https://ai.google.dev/gemini-api/docs/text-generation\n",
    "    # https://ai.google.dev/api/generate-content#method:-models.generatecontent\n",
    "\n",
    "    model_name = \"gemini-2.5-pro-exp-03-25\"\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    request_body = {\n",
    "        system_prompt_key: {\n",
    "            \"parts\": [{\"text\": system_prompt}],\n",
    "        },\n",
    "        \"contents\": [\n",
    "            {\"parts\": [{\"text\": user_prompt}]},\n",
    "        ],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxOutputTokens\": max_out_tokens,\n",
    "            \"responseModalities\": [\"text\"],\n",
    "            \"thinkingConfig\": {\n",
    "                \"includeThoughts\": True,\n",
    "                \"thinkingBudget\": reasoning_tokens,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    url = \"\".join(\n",
    "        (\n",
    "            \"https://generativelanguage.googleapis.com/v1beta/models/\",\n",
    "            urllib.parse.quote_plus(model_name),\n",
    "            \":generateContent?key=\",\n",
    "            urllib.parse.quote_plus(api_keys[\"google\"]),\n",
    "        )\n",
    "    )\n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        url,\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[],\n",
    "        sensitive_body_fields=[],\n",
    "    )\n",
    "\n",
    "    for candidate in get_item(result, \"response.body.candidates\"):\n",
    "        if get_item(candidate[\"content\"], \"role\") == \"model\":\n",
    "            for part in get_item(candidate, \"content.parts\"):\n",
    "                text = get_item(part, \"text\")\n",
    "\n",
    "                if text is not None and not get_item(part, \"thought\"):\n",
    "                    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f1a88-bc76-4f41-8edb-fc985de78188",
   "metadata": {},
   "source": [
    "As of May, 2025, some of the API documentation of Gemini uses\n",
    "[snake_case](https://ai.google.dev/gemini-api/docs/text-generation#system-instructions)\n",
    "for the system prompt field, other parts of the documentation use\n",
    "[camelCase](https://ai.google.dev/api/generate-content#method:-models.generatecontent).\n",
    "The code below attempts to use both in order to see if any or both\n",
    "can be accepted by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406c7e34-6c1d-4b86-ab2b-64c51c78700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# system_instruction:\n",
      "Aye, the wee bits o' air scatter the blue sunlight about more than the red, makin' the heavens look that fine azure color!\n",
      "\n",
      "# systemInstruction:\n",
      "Arrr, the wee bits o' air scatter the blue sunlight 'round more than the other colors, makin' the heavens look that fine shade!\n"
     ]
    }
   ],
   "source": [
    "print(\"# system_instruction:\")\n",
    "print(\n",
    "    query_gemini(\n",
    "        'pirate-snake_case',\n",
    "        \"Talk like a pirate.\",\n",
    "        \"Explain in one brief sentence why the sky is blue.\",\n",
    "        system_prompt_key=\"system_instruction\",\n",
    "    )\n",
    ")\n",
    "print(\"\")\n",
    "print(\"# systemInstruction:\")\n",
    "print(\n",
    "    query_gemini(\n",
    "        'pirate-camelCase',\n",
    "        \"Talk like a pirate.\",\n",
    "        \"Explain in one brief sentence why the sky is blue.\",\n",
    "        system_prompt_key=\"systemInstruction\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4bf39-cb57-4a35-9b6c-ad7e87043eea",
   "metadata": {},
   "source": [
    "#### OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c6f1687-1968-4125-aae8-7f727edbf45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(\n",
    "        model_name: str,\n",
    "        accepts_temperature: bool,\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    # https://platform.openai.com/docs/guides/text?api-mode=responses\n",
    "    # https://platform.openai.com/docs/api-reference/responses/create\n",
    "\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + api_keys[\"openai\"],\n",
    "    }\n",
    "    request_body = {\n",
    "        \"model\": model_name,\n",
    "        \"max_output_tokens\": max_out_tokens,\n",
    "        \"input\": [\n",
    "            {\"role\": \"developer\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    if accepts_temperature:\n",
    "        request_body[\"temperature\"] = temperature\n",
    "    \n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        \"https://api.openai.com/v1/responses\",\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[\"Authorization\", \"openai-organization\", \"x-request-id\", \"Set-Cookie\", \"CF-RAY\"],\n",
    "        sensitive_body_fields=[\"id\", \"output.*.id\"],\n",
    "    )\n",
    "\n",
    "    for output in get_item(result, \"response.body.output\"):\n",
    "        if get_item(output, \"type\") == \"message\" and get_item(output, \"role\") == \"assistant\":\n",
    "            for content in get_item(output, \"content\", []):\n",
    "                if get_item(content, \"type\") == \"output_text\":\n",
    "                    return get_item(content, \"text\")\n",
    "\n",
    "\n",
    "query_gpt4 = functools.partial(query_openai, \"gpt-4.1-2025-04-14\", True)\n",
    "query_o3mini = functools.partial(query_openai, \"o3-mini-2025-01-31\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20350c-e7d6-4cdf-b2a6-1dce8b86d2da",
   "metadata": {},
   "source": [
    "#### Perplexity AI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e227ebb-7bf0-46aa-91ba-ef5264aa3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_perplexity(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    # https://docs.perplexity.ai/guides/getting-started\n",
    "    # https://docs.perplexity.ai/api-reference/chat-completions\n",
    "\n",
    "    model_name = \"sonar-reasoning-pro\"\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + api_keys[\"perplexity\"],\n",
    "    }\n",
    "    request_body = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"max_tokens\": max_out_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"return_related_questions\": False,\n",
    "        \"stream\": False,\n",
    "        \"web_search_options\": {\n",
    "            \"search_context_size\": \"low\",\n",
    "        },\n",
    "    }\n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        \"https://api.perplexity.ai/chat/completions\",\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[\"Authorization\", \"Set-Cookie\", \"CF-RAY\", ],\n",
    "        sensitive_body_fields=[\"id\", ],\n",
    "    )\n",
    "\n",
    "    for choice in get_item(result, \"response.body.choices\"):\n",
    "        if get_item(choice, \"message.role\") == \"assistant\":\n",
    "            return get_item(choice, \"message.content\")\n",
    "\n",
    "    return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
