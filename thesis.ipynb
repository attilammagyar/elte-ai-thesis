{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8b718b-971a-49c5-8b17-8c7f1897f43e",
   "metadata": {},
   "source": [
    "Investigating Bias in LLM Self-Evaluation\n",
    "========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642a8f0-c554-45b7-a1b8-3c5c27fbad17",
   "metadata": {},
   "source": [
    "Appendix\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75908374-5f66-4f8d-8d80-6959e4d5fdb2",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf130a-98e4-4545-b14d-436e6b644871",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7d605f9-e6b5-4f40-aaa9-5c228a4afb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib==3.10.0\n",
    "# !pip install numpy==2.2.3\n",
    "# !pip install pandas==2.2.3\n",
    "# !pip install requests==2.32.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf484201-00a8-4d73-b647-e8642c18034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as coll\n",
    "import collections.abc as collabc\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import os.path\n",
    "import typing\n",
    "import urllib.parse\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901b8e3-82c3-4179-a2f4-893e3b58ca45",
   "metadata": {},
   "source": [
    "#### API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ff7580-cccd-41eb-a8b1-14fff16eea11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys: anthropic, deepseek, google, openai, perplexity\n"
     ]
    }
   ],
   "source": [
    "api_keys_filename = \"api-keys.json\"\n",
    "\n",
    "if not os.path.isfile(api_keys_filename):\n",
    "    raise RuntimeError(f\"API keys file not found: {api_keys_filename!r}\")\n",
    "\n",
    "with open(api_keys_filename, \"r\") as f:\n",
    "    api_keys = json.load(f)\n",
    "\n",
    "\n",
    "print(\"API keys: \" + \", \".join(sorted(api_keys.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59350fba-28af-4bd0-b639-76bcbc210565",
   "metadata": {},
   "source": [
    "#### Common Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b0c381-8122-4679-85d8-1987dbe9cc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OUT_TOKENS = 32768\n",
    "MAX_REASONING_TOKENS = 8192\n",
    "TEMPERATURE = 0.3\n",
    "\n",
    "\n",
    "def query_all(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    models = {\n",
    "        \"sonnet\": query_claude_sonnet,\n",
    "        \"deepseek\": query_deepseek,\n",
    "        \"gemini\": query_gemini,\n",
    "        \"gpt4\": query_gpt4,\n",
    "        \"perplexity\": query_perplexity,\n",
    "        #\"o3mini\": query_o3mini,\n",
    "    }\n",
    "\n",
    "    for model_name, query_model in models.items():\n",
    "        response = query_model(\n",
    "            experiment_name,\n",
    "            system_prompt,\n",
    "            user_prompt,\n",
    "            temperature,\n",
    "            max_out_tokens,\n",
    "            reasoning_tokens,\n",
    "        )\n",
    "\n",
    "        yield model_name, response\n",
    "\n",
    "\n",
    "def send_request(\n",
    "        cache_filename: str,\n",
    "        url: str,\n",
    "        request_headers: collabc.Mapping,\n",
    "        request_body: collabc.Mapping,\n",
    "        sensitive_headers: collabc.Container=(),\n",
    "        sensitive_body_fields: collabc.Container=(),\n",
    "):\n",
    "    sensitive_headers = {h.lower() for h in sensitive_headers}\n",
    "    sensitive_body_fields = {f.lower() for f in sensitive_body_fields}\n",
    "\n",
    "    cache_dir = os.path.dirname(cache_filename)\n",
    "\n",
    "    if not os.path.isdir(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    \n",
    "    if os.path.isfile(cache_filename):\n",
    "        with open(cache_filename, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=request_headers, json=request_body)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        result = {\n",
    "            \"request\": {\n",
    "                \"headers\": del_items(request_headers, sensitive_headers),\n",
    "                \"body\": del_items(request_body, sensitive_body_fields),\n",
    "            },\n",
    "            \"response\": {\n",
    "                \"headders\": del_items(response.headers, sensitive_headers),\n",
    "                \"body\": del_items(response.json(), sensitive_body_fields),\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(cache_filename, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as exc:\n",
    "        print(f\"Exception: ({type(exc)}) {exc}\")\n",
    "\n",
    "        if hasattr(exc, \"response\") and exc.response is not None:\n",
    "            print(f\"Response status code: {exc.response.status_code}\")\n",
    "            print(f\"Response body: {exc.response.text}\")\n",
    "\n",
    "        raise\n",
    "\n",
    "\n",
    "def build_cache_filename(experiment_name: str, model_name: str, temperature: float):\n",
    "    return os.path.join(\n",
    "        \"cache\",\n",
    "        (f\"{experiment_name}-{model_name}-t{temperature:.3f}\".replace(\".\", \"_\")) + \".json\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_item(container, path: str, default=None):\n",
    "    if path == \".\" or path == \"\":\n",
    "        return container\n",
    "\n",
    "    path = path.split(\".\")\n",
    "\n",
    "    for key in path:\n",
    "        if isinstance(container, collabc.Mapping):\n",
    "            if key in container:\n",
    "                container = container[key]\n",
    "            else:\n",
    "                return default\n",
    "        elif isinstance(container, collabc.Sequence):\n",
    "            if int(key) < len(container):\n",
    "                container = container[int(key)]\n",
    "            else:\n",
    "                return default\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    return container\n",
    "\n",
    "\n",
    "def del_items(container, patterns: typing.List[str]):\n",
    "    def should_include(path: list, exclude_patterns: typing.List[tuple]) -> bool:\n",
    "        return not any(path_matches_pattern(path, ptrn) for ptrn in exclude_patterns)\n",
    "\n",
    "    def copy_recursive(obj, path: list, exclude_patterns: typing.List[tuple]):\n",
    "        if isinstance(obj, str):\n",
    "            return obj\n",
    "\n",
    "        if isinstance(obj, collabc.Mapping):\n",
    "            copy = {}\n",
    "\n",
    "            for k, v in obj.items():\n",
    "                path_ext = path + [k]\n",
    "\n",
    "                if should_include(path_ext, exclude_patterns):\n",
    "                    copy[k] = copy_recursive(v, path_ext, exclude_patterns)\n",
    "\n",
    "            return copy\n",
    "\n",
    "        if isinstance(obj, collabc.Sequence):\n",
    "            copy = []\n",
    "\n",
    "            for k, v in enumerate(obj):\n",
    "                path_ext = path + [str(k)]\n",
    "\n",
    "                if should_include(path_ext, exclude_patterns):\n",
    "                    copy.append(copy_recursive(v, path_ext, exclude_patterns))\n",
    "\n",
    "            return copy\n",
    "\n",
    "        return obj\n",
    "\n",
    "    for pattern in patterns:\n",
    "        if pattern == \".\" or pattern == \"\":\n",
    "            return ValueError(f\"Invalid pattern; {pattern=!r}\")\n",
    "\n",
    "    patterns = [tuple(pattern.lower().split(\".\")) for pattern in patterns]\n",
    "    \n",
    "    return copy_recursive(container, [], patterns)\n",
    "\n",
    "\n",
    "def path_matches_pattern(path: collabc.Sequence, pattern: collabc.Sequence) -> bool:\n",
    "    if len(path) != len(pattern):\n",
    "        return False\n",
    "\n",
    "    for path_component, pattern_component in zip(path, pattern):\n",
    "        matches = (\n",
    "            pattern_component == \"*\"\n",
    "            or pattern_component == path_component.lower()\n",
    "        )\n",
    "\n",
    "        if not matches:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_get_item():\n",
    "    container = {\"aaa\": [{\"bbb\": \"42\", \"ccc\": \"123\"}]}\n",
    "\n",
    "    assert_eq(\"42\", get_item(container, \"aaa.0.bbb\"))\n",
    "    assert_eq(None, get_item(container, \"aaa.2.zzz\"))\n",
    "\n",
    "\n",
    "def test_del_item():\n",
    "    container = {\"aaa\": [{\"bbb\": \"42\", \"ccc\": \"123\", \"ddd\": \"hello\"}]}\n",
    "\n",
    "    assert_eq({\"aaa\": [{\"ddd\": \"hello\"}]}, del_items(container, [\"aaa.*.ccc\", \"*.*.bbb\", \"zzz\"]))\n",
    "\n",
    "\n",
    "def assert_eq(a, b):\n",
    "    assert a == b, f\"Failed to assert that a = b; {a=!r}, {b=!r}\"\n",
    "\n",
    "\n",
    "test_get_item()\n",
    "test_del_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfc9bf-ac9f-4b9c-abd8-6377c9a0e841",
   "metadata": {},
   "source": [
    "#### Anthropic Claude Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c43666-4d0c-41ec-97b4-280e1b2c820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_claude_sonnet(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    # https://docs.anthropic.com/en/api/messages\n",
    "\n",
    "    model_name = \"claude-3-7-sonnet-20250219\"\n",
    "    temperature = 1  # Thinking requires temperature to be 1.\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"x-api-key\": api_keys[\"anthropic\"],\n",
    "        \"anthropic-version\": \"2023-06-01\",\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "    request_body = {\n",
    "        \"model\": model_name,\n",
    "        \"max_tokens\": max_out_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stream\": False,\n",
    "        \"system\": system_prompt,\n",
    "        \"thinking\": {\n",
    "            \"type\": \"enabled\",\n",
    "            \"budget_tokens\": reasoning_tokens,\n",
    "        },\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        \"https://api.anthropic.com/v1/messages\",\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[\"x-api-key\", \"anthropic-organization-id\", \"request-id\", \"CF-RAY\"],\n",
    "        sensitive_body_fields=[\"id\"],\n",
    "    )\n",
    "\n",
    "    for content in get_item(result, \"response.body.content\"):\n",
    "        if get_item(content, \"type\") == \"text\":\n",
    "            return content[\"text\"]\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdd9ca0-8d56-410c-827b-e554a8fb76ab",
   "metadata": {},
   "source": [
    "#### DeepSeek Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ef3461-d8fa-40cd-9d7e-f5c75abb5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deepseek(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    # https://api-docs.deepseek.com/api/create-chat-completion\n",
    "\n",
    "    max_out_tokens = min(8192, max_out_tokens)\n",
    "    reasoning_tokens = min(max_out_tokens // 2 + 1, reasoning_tokens)\n",
    "\n",
    "    model_name = \"deepseek-chat\"\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + api_keys[\"deepseek\"],\n",
    "    }\n",
    "    request_body = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"max_tokens\": max_out_tokens,\n",
    "        \"response_format\": {\"type\": \"text\"},\n",
    "        \"stream\": False,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        \"https://api.deepseek.com/chat/completions\",\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[\"Authorization\", \"Set-Cookie\", \"x-ds-trace-id\", \"CF-RAY\"],\n",
    "        sensitive_body_fields=[\"id\"],\n",
    "    )\n",
    "\n",
    "    for choice in get_item(result, \"response.body.choices\"):\n",
    "        if get_item(choice, \"message.role\") == \"assistant\":\n",
    "            return get_item(choice, \"message.content\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708d514-7435-4224-a165-446ba437eb42",
   "metadata": {},
   "source": [
    "#### Google Gemini Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6feceb38-3d2a-4bb8-95f0-41b41db13c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gemini(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "        system_prompt_key: str=\"systemInstruction\",\n",
    "):\n",
    "    # https://ai.google.dev/gemini-api/docs/text-generation\n",
    "    # https://ai.google.dev/api/generate-content#method:-models.generatecontent\n",
    "\n",
    "    model_name = \"gemini-2.5-pro-exp-03-25\"\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    request_body = {\n",
    "        system_prompt_key: {\n",
    "            \"parts\": [{\"text\": system_prompt}],\n",
    "        },\n",
    "        \"contents\": [\n",
    "            {\"parts\": [{\"text\": user_prompt}]},\n",
    "        ],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxOutputTokens\": max_out_tokens,\n",
    "            \"responseModalities\": [\"text\"],\n",
    "            \"thinkingConfig\": {\n",
    "                \"includeThoughts\": True,\n",
    "                \"thinkingBudget\": reasoning_tokens,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    url = \"\".join(\n",
    "        (\n",
    "            \"https://generativelanguage.googleapis.com/v1beta/models/\",\n",
    "            urllib.parse.quote_plus(model_name),\n",
    "            \":generateContent?key=\",\n",
    "            urllib.parse.quote_plus(api_keys[\"google\"]),\n",
    "        )\n",
    "    )\n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        url,\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[],\n",
    "        sensitive_body_fields=[],\n",
    "    )\n",
    "\n",
    "    for candidate in get_item(result, \"response.body.candidates\"):\n",
    "        if get_item(candidate[\"content\"], \"role\") == \"model\":\n",
    "            for part in get_item(candidate, \"content.parts\"):\n",
    "                text = get_item(part, \"text\")\n",
    "\n",
    "                if text is not None and not get_item(part, \"thought\"):\n",
    "                    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f1a88-bc76-4f41-8edb-fc985de78188",
   "metadata": {},
   "source": [
    "As of May, 2025, some of the API documentation of Gemini uses\n",
    "[snake_case](https://ai.google.dev/gemini-api/docs/text-generation#system-instructions)\n",
    "for the system prompt field, other parts of the documentation use\n",
    "[camelCase](https://ai.google.dev/api/generate-content#method:-models.generatecontent).\n",
    "The code below attempts to use both in order to see if any or both\n",
    "can be accepted by the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406c7e34-6c1d-4b86-ab2b-64c51c78700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# system_instruction:\n",
      "Aye, the wee bits o' air scatter the blue sunlight about more than the red, makin' the heavens look that fine azure color!\n",
      "\n",
      "# systemInstruction:\n",
      "Arrr, the wee bits o' air scatter the blue sunlight 'round more than the other colors, makin' the heavens look that fine shade!\n"
     ]
    }
   ],
   "source": [
    "print(\"# system_instruction:\")\n",
    "print(\n",
    "    query_gemini(\n",
    "        'pirate-snake_case',\n",
    "        \"Talk like a pirate.\",\n",
    "        \"Explain in one brief sentence why the sky is blue.\",\n",
    "        system_prompt_key=\"system_instruction\",\n",
    "    )\n",
    ")\n",
    "print(\"\")\n",
    "print(\"# systemInstruction:\")\n",
    "print(\n",
    "    query_gemini(\n",
    "        'pirate-camelCase',\n",
    "        \"Talk like a pirate.\",\n",
    "        \"Explain in one brief sentence why the sky is blue.\",\n",
    "        system_prompt_key=\"systemInstruction\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4bf39-cb57-4a35-9b6c-ad7e87043eea",
   "metadata": {},
   "source": [
    "#### OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c6f1687-1968-4125-aae8-7f727edbf45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai(\n",
    "        model_name: str,\n",
    "        accepts_temperature: bool,\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    # https://platform.openai.com/docs/guides/text?api-mode=responses\n",
    "    # https://platform.openai.com/docs/api-reference/responses/create\n",
    "\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + api_keys[\"openai\"],\n",
    "    }\n",
    "    request_body = {\n",
    "        \"model\": model_name,\n",
    "        \"max_output_tokens\": max_out_tokens,\n",
    "        \"input\": [\n",
    "            {\"role\": \"developer\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    if accepts_temperature:\n",
    "        request_body[\"temperature\"] = temperature\n",
    "    \n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        \"https://api.openai.com/v1/responses\",\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[\"Authorization\", \"openai-organization\", \"x-request-id\", \"Set-Cookie\", \"CF-RAY\"],\n",
    "        sensitive_body_fields=[\"id\", \"output.*.id\"],\n",
    "    )\n",
    "\n",
    "    for output in get_item(result, \"response.body.output\"):\n",
    "        if get_item(output, \"type\") == \"message\" and get_item(output, \"role\") == \"assistant\":\n",
    "            for content in get_item(output, \"content\", []):\n",
    "                if get_item(content, \"type\") == \"output_text\":\n",
    "                    return get_item(content, \"text\")\n",
    "\n",
    "\n",
    "query_gpt4 = functools.partial(query_openai, \"gpt-4.1-2025-04-14\", True)\n",
    "query_o3mini = functools.partial(query_openai, \"o3-mini-2025-01-31\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20350c-e7d6-4cdf-b2a6-1dce8b86d2da",
   "metadata": {},
   "source": [
    "#### Perplexity Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e227ebb-7bf0-46aa-91ba-ef5264aa3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_perplexity(\n",
    "        experiment_name: str,\n",
    "        system_prompt: str,\n",
    "        user_prompt: str,\n",
    "        temperature: float=TEMPERATURE,\n",
    "        max_out_tokens: int=MAX_OUT_TOKENS,\n",
    "        reasoning_tokens: int=MAX_REASONING_TOKENS,\n",
    "):\n",
    "    # https://docs.perplexity.ai/guides/getting-started\n",
    "    # https://docs.perplexity.ai/api-reference/chat-completions\n",
    "\n",
    "    model_name = \"sonar-reasoning-pro\"\n",
    "    cache_filename = build_cache_filename(experiment_name, model_name, temperature)\n",
    "    request_headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + api_keys[\"perplexity\"],\n",
    "    }\n",
    "    request_body = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"max_tokens\": max_out_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"return_related_questions\": False,\n",
    "        \"stream\": False,\n",
    "        \"web_search_options\": {\n",
    "            \"search_context_size\": \"low\",\n",
    "        },\n",
    "    }\n",
    "    result = send_request(\n",
    "        cache_filename,\n",
    "        \"https://api.perplexity.ai/chat/completions\",\n",
    "        request_headers,\n",
    "        request_body,\n",
    "        sensitive_headers=[\"Authorization\", \"Set-Cookie\", \"CF-RAY\", ],\n",
    "        sensitive_body_fields=[\"id\", ],\n",
    "    )\n",
    "\n",
    "    for choice in get_item(result, \"response.body.choices\"):\n",
    "        if get_item(choice, \"message.role\") == \"assistant\":\n",
    "            return get_item(choice, \"message.content\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d2e26-382d-49ee-a204-7960a0184907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
