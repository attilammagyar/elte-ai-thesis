@article{attention,
    author={
        Ashish Vaswani and
        Noam Shazeer and
        Niki Parmar and
        Jakob Uszkoreit and
        Llion Jones and
        Aidan N. Gomez and
        Lukasz Kaiser and
        Illia Polosukhin
    },
    title={Attention Is All You Need},
    journal={CoRR},
    volume={abs/1706.03762},
    year={2017},
    doi={https://doi.org/10.48550/arXiv.1706.03762}
}

@article{gpt3,
    author={OpenAI},
    title={Language Models are Few-Shot Learners},
    journal={CoRR},
    volume={abs/2005.14165},
    year={2020},
    doi={https://doi.org/10.48550/arXiv.2005.14165}
}

@article{gpt4,
    author={OpenAI},
    title={GPT-4 Technical Report},
    year={2023},
    doi={https://doi.org/10.48550/arXiv.2303.08774}
}

@book{llms,
    author={Tong Xiao and Jingbo Zhu},
    title={Foundations of Large Language Models},
    year={2025},
    doi={https://doi.org/10.48550/arXiv.2501.09223}
}

@article{cot,
    author={
        Jason Wei and
        Xuezhi Wang and
        Dale Schuurmans and
        Maarten Bosma and
        Brian Ichter and
        Fei Xia and
        Ed Chi and
        Quoc Le and
        Denny Zhou
    },
    title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
    journal={CoRR},
    volume={abs/2201.11903},
    year={2022},
    doi={https://doi.org/10.48550/arXiv.2201.11903}
}
